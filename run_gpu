#!/bin/bash
#RUN using the following command template
#./run_gpu zork <scenario> <agent_type> [GPU_ID] [RANDOM_SEED]

GAME=$1
SCENAR=$2
TWEAK=$3
GPU=${4:-0}
SEED=${5:-0}
FRAMEWORK="ZorkFramework"
agent="NeuralQLearner"
#####################################################
agent_tweak=$TWEAK              #"greedy" #"explore" #"merged" #"vanila"
scenario=$SCENAR                #select scenarion: #4 - 200 objects, troll quest #3 200 objects, egg #2 - 20 objects, short egg quest. #1 - 5 objects, short egg quest
netfile="conv_q_net"            #DQN arch file
obj_net_file="conv_obj_net"     #AEN arch file
obj_start=1                     #number of learning starts until the agent can use object net insight
obj_bad_cmd_thresh=0.6          #when sigmoid output is larger then the threshold consider it as bad prediction
obj_acc_trust_thresh=0.75       #accuracy threshold for agent to use object net insight
obj_drop_prob=0.9               #probability to drop actions which Obj net condsiders as bad for explore mod
if [ $scenario -gt 1 ]; then
  obj_sample=5                  #[0,n_objects] how many objects to sample
  obj_max=5                     #[0,n_objects] how many most likely objects to consider with greedy action wp 1
else 
  obj_sample=1                    
  obj_max=2                      
fi
bad_parse_penalty_scale=0       #apply penalty multiplier for bad commands
lr=0.0025                       #DQN learning rate
obj_lr=0.00043                  #AEN learning rate
discount=0.8
learn_start=50000
replay_memory=1000000
steps=2000000
eval_freq=10000                 # can change ratio between steps and eval_freq to get more samples
eval_steps=2000
prog_freq=10000
save_freq=100000
eps_end=0.05
eps_endt=500000
gpu=$GPU
seed=$SEED
verbose=2
bufferSize=1024                 #512
valid_size=500                  #500
minibatch_size=32               #32
#####################################################

agent_name=$GAME"_scenario_"$scenario"_"$agent_tweak"_Q-arch_"$netfile"_seed_"$SEED
if [ $agent_tweak != "vanila" ]; then
  agent_name=$agent_name"_AEN-arch_"$obj_net_file
  if [ $agent_tweak != "explore" ]; then
    agent_name=$agent_name"_max_"$obj_max"_sample_"$obj_sample
  fi
  if [ $agent_tweak != "greedy" ]; then
    agent_name=$agent_name"_drop_prob_"$obj_drop_prob
  fi
fi
if [ $bad_parse_penalty_scale -gt 0 ]; then 
  agent_name=$agent_name"_bp-penalty_"$bad_parse_penalty_scale
fi

env_params="bad_parse_penalty_scale="$bad_parse_penalty_scale",game_scenario="$scenario""
game_path=$PWD"/dqn/zork"
n_replay=1
update_freq=4
actrep=1
random_starts=0
num_threads=4
pool_frms_type="\"max\""
pool_frms_size=2
pool_frms="type="$pool_frms_type",size="$pool_frms_size
initial_priority="false"
state_rows=65  #@NOTE we added the folowing 2 arguments to allow neural q learner know what are the input dimms
state_cols=300
state_dim=19500 #@DEBUG_DIM(state(sentence)size*word representation)
ncols=1 # number of color channels we only have 1
#nrows=3

agent_params="obj_lr="$obj_lr",lr="$lr",ep=1,ep_end="$eps_end",ep_endt="$eps_endt",discount="$discount",hist_len=4,learn_start="$learn_start",replay_memory="$replay_memory",update_freq="$update_freq",n_replay="$n_replay",network="\"$netfile\"",agent_tweak=\"$agent_tweak\",state_dim="$state_dim",minibatch_size="$minibatch_size",rescale_r=1,ncols="$ncols",state_rows="$state_rows",state_cols="$state_cols",bufferSize="$bufferSize",valid_size="$valid_size",target_q=10000,clip_delta=1,min_reward=-1,max_reward=99,obj_bad_cmd_thresh="$obj_bad_cmd_thresh",obj_start="$obj_start",obj_thresh_acc="$obj_acc_trust_thresh",obj_sample="$obj_sample",obj_max="$obj_max",obj_drop_prob="$obj_drop_prob",obj_net_file="\"$obj_net_file\"""
args="-verbose $verbose -framework $FRAMEWORK -game_path $game_path -name $agent_name -env $GAME -env_params $env_params -agent $agent -agent_params $agent_params -steps $steps -eval_freq $eval_freq -eval_steps $eval_steps -prog_freq $prog_freq -save_freq $save_freq -actrep $actrep -gpu $gpu -random_starts $random_starts -pool_frms $pool_frms -seed $seed -threads $num_threads"
echo $args

cd dqn
qlua train_agent.lua $args